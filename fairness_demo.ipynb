{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from collections import Counter\n",
    "import cvxpy as cp\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from tqdm.notebook import tqdm\n",
    "import higher\n",
    "import pandas as pd\n",
    "import lale.lib.aif360\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    \n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"bank\"\n",
    "# \"adult\", bank, us_crime, student_por, default_credit, titanic\"\n",
    "sensitive = 0\n",
    "\n",
    "class Args:\n",
    "    res_epochs = 50\n",
    "    momentum = 0.9\n",
    "    weight_decay = 5e-4\n",
    "    num_seeds = 5\n",
    "    if dataset == 'adult' and sensitive == 1: # sex\n",
    "        lr = 1e-3\n",
    "        p_drop = 0.2\n",
    "        batch_size = 512\n",
    "        bargain_epochs = 15\n",
    "    elif dataset == 'adult' and sensitive == 0: # race\n",
    "        lr = 5e-4\n",
    "        p_drop = 0.4\n",
    "        batch_size = 512\n",
    "        bargain_epochs = 15\n",
    "    elif dataset == 'bank':\n",
    "        lr = 1e-3 \n",
    "        p_drop = 0.3\n",
    "        batch_size = 512\n",
    "        bargain_epochs = 15\n",
    "    elif dataset == 'us_crime':\n",
    "        lr = 1e-4 \n",
    "        p_drop = 0.2\n",
    "        batch_size = 32 \n",
    "        bargain_epochs = 15\n",
    "    elif dataset == 'default_credit':\n",
    "        res_epochs = 100\n",
    "        lr = 1e-3\n",
    "        p_drop = 0.5\n",
    "        batch_size = 512\n",
    "        bargain_epochs = 100\n",
    "    elif dataset == 'student_por':\n",
    "        lr = 1e-3 \n",
    "        p_drop = 0.05\n",
    "        batch_size = 32\n",
    "        bargain_epochs = 10\n",
    "    elif dataset == 'titanic':\n",
    "        lr = 1e-3 \n",
    "        p_drop = 0.4\n",
    "        batch_size = 128\n",
    "        bargain_epochs = 50\n",
    "args=Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names = {\n",
    "    \"adult\": \"adult\",\n",
    "    \"bank\": \"bank\",\n",
    "    \"default_credit\": \"default_credit\",\n",
    "    \"titanic\": \"titanic\",\n",
    "    \"student_por\": \"student_por\",\n",
    "    \"tae\": \"tae\",\n",
    "    \"us_crime\": \"us_crime\",\n",
    "}\n",
    "\n",
    "def try_fetch(dataset_name):\n",
    "    long_name = dataset_names[dataset_name]\n",
    "    fetcher_function = getattr(lale.lib.aif360.datasets, f\"fetch_{long_name}_df\")\n",
    "    try:\n",
    "        X, y, fairness_info = fetcher_function()\n",
    "    except SystemExit:\n",
    "        print(f\"skipping {dataset_name} because it is not downloaded\")\n",
    "        return None\n",
    "    return X, y, fairness_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pre, y, fairness_info = try_fetch(dataset)\n",
    "y_true = np.asarray((y == fairness_info['favorable_labels'][0]).astype(int))\n",
    "n_classes = y_true.max()+1\n",
    "X_pre = X_pre.reset_index(drop=True)\n",
    "sesitive_feature = [fairness_info['protected_attributes'][i]['feature'] for i in range(len(fairness_info['protected_attributes']))]\n",
    "if dataset == 'titanic':\n",
    "    X_pre.drop(columns=\"name\", inplace=True)\n",
    "    X_pre = X_pre.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start with the first sensitive feature for experiment\n",
    "current_sensitive = sesitive_feature[sensitive]\n",
    "sensitive_attr = X_pre[current_sensitive]\n",
    "\n",
    "if current_sensitive == 'age':\n",
    "    sensitive_attr = sensitive_attr > fairness_info['protected_attributes'][sensitive]['reference_group'][0][0]\n",
    "    sensitive_attr = sensitive_attr.astype(int)\n",
    "    sensitive_attr = sensitive_attr.rename('age')\n",
    "    \n",
    "X = pd.get_dummies(X_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.sort_index(axis=1)\n",
    "unique_groups = sensitive_attr.unique()\n",
    "indict = np.arange(sensitive_attr.shape[0])\n",
    "scaler = StandardScaler().fit(X)\n",
    "X = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_combined_arrays(file_path):\n",
    "    \"\"\"\n",
    "    Load three numpy arrays from a single file.\n",
    "\n",
    "    Parameters:\n",
    "    file_path (str): The path of the file containing the arrays.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing three numpy arrays.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the arrays from the file\n",
    "    data = np.load(file_path)\n",
    "    return data['ind_train'], data['ind_test'], data['val_idx']\n",
    "\n",
    "ind_train, ind_test, val_idx = load_combined_arrays(f'./data_idx/{dataset}/{current_sensitive}/combined_arrays.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, X_val = X[ind_train], X[ind_test], X[val_idx]\n",
    "y_train, y_test, y_val = y_true[ind_train], y_true[ind_test], y_true[val_idx]\n",
    "s_train, s_test, s_val = sensitive_attr[ind_train].values, sensitive_attr[ind_test].values, sensitive_attr[val_idx].values\n",
    "val_size = len(val_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nTrain Distribution')\n",
    "print(sensitive_attr[ind_train].value_counts())\n",
    "for i in range(len(unique_groups)):\n",
    "    print(unique_groups[i], Counter(y_train[sensitive_attr[ind_train] == unique_groups[i]]))\n",
    "\n",
    "\n",
    "print('\\nTest Distribution')\n",
    "print(sensitive_attr[ind_test].value_counts())\n",
    "for i in range(len(unique_groups)):\n",
    "    print(unique_groups[i], Counter(y_test[sensitive_attr[ind_test] == unique_groups[i]]))\n",
    "\n",
    "\n",
    "\n",
    "print('\\nVal(Meta) Distribution')\n",
    "print(sensitive_attr[val_idx].value_counts())\n",
    "print(Counter(y_val))\n",
    "for i in range(len(unique_groups)):\n",
    "    print(unique_groups[i], Counter(y_val[sensitive_attr[val_idx] == unique_groups[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PandasDataSet(TensorDataset):\n",
    "    \n",
    "    def __init__(self, *dataframes):\n",
    "        tensors = (self._df_to_tensor(df) for df in dataframes)\n",
    "        super(PandasDataSet, self).__init__(*tensors)\n",
    "\n",
    "    def _df_to_tensor(self, df):\n",
    "        if isinstance(df, np.ndarray):\n",
    "            return torch.from_numpy(df).float()\n",
    "        return torch.from_numpy(df.values).float()\n",
    "\n",
    "    \n",
    "train_data = PandasDataSet(X_train, y_train, ind_train)\n",
    "train_loader = DataLoader(train_data, batch_size=args.batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fair_eval(y_test,y_pred_labels,unique_groups):\n",
    "    AUC = []\n",
    "    TPR = []\n",
    "    FPR = []\n",
    "    F1 = []\n",
    "    for group in unique_groups:\n",
    "        mask = sensitive_attr[ind_test] == group\n",
    "        auc_roc = roc_auc_score(y_test[mask], y_pred_labels[mask])\n",
    "        AUC.append(auc_roc)\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test[mask], y_pred_labels[mask]).ravel()\n",
    "        tpr = tp / (tp + fn + 1e-8)  # True Positive Rate\n",
    "        TPR.append(tpr)\n",
    "        fpr = fp / (fp + tn + 1e-8)  # False Positive Rate\n",
    "        FPR.append(fpr)\n",
    "        F1.append(2*tp/(2*tp+fn+fp))\n",
    "    # other_mask = sensitive_attr[ind_test] != unique_groups[AUC.index(min(AUC))]\n",
    "    # other_AUC = roc_auc_score(y_test[other_mask], y_pred_labels[other_mask])\n",
    "    return AUC, TPR, FPR, F1 #, other_AUC\n",
    "\n",
    "n_hid = 64 #512\n",
    "def bargaining_fail_eval(grads, overall_grad):\n",
    "    similarity = np.asarray([torch.dot(overall_grad,v).item() for v in grads.values()])\n",
    "    if (similarity <= 0).sum():\n",
    "        return 1, similarity\n",
    "    else:\n",
    "        return 0, similarity\n",
    "\n",
    "bargain_groups = 2*len(unique_groups)\n",
    "\n",
    "def mean_std(group_mets):\n",
    "    means = np.mean(np.asarray(group_mets), axis=0)\n",
    "    stds = np.std(np.asarray(group_mets), axis=0)\n",
    "    return(np.asarray(list(zip(np.min(means, axis=1), stds[np.arange(len(means)), np.argmin(means, axis=1)]))))\n",
    "\n",
    "def _stop_criteria(gtg, alpha_t):\n",
    "    return (\n",
    "        (alpha_param.value is None)\n",
    "        or (np.linalg.norm(gtg @ alpha_t - 1 / (alpha_t + 1e-10)) < 1e-3)\n",
    "        or (\n",
    "            np.linalg.norm(alpha_param.value - prvs_alpha_param.value)\n",
    "            < 1e-3\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def return_weights(grads, prvs_alpha):\n",
    "    G = torch.stack(tuple(v for v in grads.values()))\n",
    "    GTG = torch.mm(G, G.t())\n",
    "    normalization_factor = (\n",
    "        torch.norm(GTG).detach().cpu().numpy().reshape((1,)) + 1e-6\n",
    "        )\n",
    "    if (np.isnan(normalization_factor) | np.isinf(normalization_factor)).any():\n",
    "        normalization_factor = np.array([1.0])\n",
    "    GTG = GTG / normalization_factor.item()\n",
    "    gtg = GTG.cpu().detach().numpy()\n",
    "    G_param.value = gtg\n",
    "    normalization_factor_param.value = normalization_factor\n",
    "\n",
    "    optim_niter=100\n",
    "    alpha_t = prvs_alpha\n",
    "    for _ in range(optim_niter):\n",
    "        try:\n",
    "            alpha_param.value = alpha_t\n",
    "            prvs_alpha_param.value = alpha_t\n",
    "            # try:\n",
    "            prob.solve(solver=cp.ECOS, warm_start=True, max_iters=100)\n",
    "        except:\n",
    "            alpha_param.value = prvs_alpha_param.value\n",
    "\n",
    "        if _stop_criteria(gtg, alpha_t):\n",
    "            break\n",
    "\n",
    "        alpha_t = alpha_param.value\n",
    "    if alpha_t is not None and not (np.isnan(alpha_t) | np.isinf(alpha_t)).any():\n",
    "        return alpha_t\n",
    "    else:\n",
    "        return prvs_alpha\n",
    "    \n",
    "    \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def F1Loss(y_f, y):\n",
    "    y = F.one_hot(y, num_classes=2)\n",
    "    activation = nn.Sigmoid()\n",
    "    #compute TP,FP,FN,TN\n",
    "    tp = activation(y_f)*y\n",
    "    fp = activation(y_f)*(1-y)\n",
    "    fn = (1-activation(y_f))*y\n",
    "    tn = (1-activation(y_f))*(1-y)\n",
    "    return activation(torch.sum(-2*tp/(2*tp+fn+fp),axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_features, n_class=2, n_hidden=64, p_dropout=args.p_drop):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(n_features, n_hidden*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p_dropout),\n",
    "            nn.Linear(n_hidden*2, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p_dropout),\n",
    "            nn.Linear(n_hidden, n_class),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def save_metrics(method, metrics, meta_methods, new_nego_rec=None):\n",
    "    \"\"\"\n",
    "    Saves the given metrics to files in a directory specific to the method.\n",
    "    \n",
    "    :param method: The name of the method, used to create a directory.\n",
    "    :param metrics: A dictionary containing the metrics to be saved.\n",
    "    :param meta_methods: A list of methods considered as meta methods.\n",
    "    :param new_nego_rec: Additional metrics specific to meta methods.\n",
    "    \"\"\"\n",
    "    # Create a directory for the method if it doesn't exist\n",
    "    directory = f\"./results/{dataset}/{current_sensitive}/{method}/{seed}\"\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    # If the method is a meta method, add new_nego_rec to the metrics\n",
    "    if method in meta_methods and new_nego_rec is not None:\n",
    "        arr = 1-np.asarray(new_nego_rec)\n",
    "        arr = np.asarray(arr.reshape(-1,len(train_loader)).sum(axis=1))/len(train_loader)\n",
    "        metrics['nego_success'] = arr\n",
    "\n",
    "    # Iterate over the metrics and save each one in a separate file\n",
    "    for metric_name, metric_values in metrics.items():\n",
    "        file_path = os.path.join(directory, f\"{metric_name}.json\")\n",
    "        with open(file_path, 'w') as file:\n",
    "            # Convert numpy arrays to lists for JSON serialization\n",
    "            if isinstance(metric_values, np.ndarray):\n",
    "                metric_values = metric_values.tolist()\n",
    "            json.dump(metric_values, file)\n",
    "\n",
    "    print(f\"Metrics for {method} saved in {directory}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classic_methods = ['baseline', 'DRO']\n",
    "meta_methods = ['loss', 'FORML', 'meta_group_dro', 'nash_loss', 'nash_meta_group_dro', 'nash_FORML']\n",
    "methods = ['baseline', 'DRO', 'FORML', 'nash_FORML', 'loss', 'nash_loss', 'meta_group_dro', 'nash_meta_group_dro']\n",
    "\n",
    "\n",
    "weight_bargain_all, weight_post_bargain_all = {}, {}\n",
    "suweight_bargain_all, suweight_post_bargain_all = {}, {}\n",
    "nego_sims_all = {}\n",
    "\n",
    "for method in methods:\n",
    "    gpu = 0\n",
    "    epoch_AUCs, group_AUCs, new_nego_recs = [], [], []\n",
    "    \n",
    "    nego_sims = []\n",
    "    weight_bargain, weight_post_bargain = [], []\n",
    "    suweight_bargain, suweight_post_bargain = [], []\n",
    "    \n",
    "    for seed in range(args.num_seeds):\n",
    "        torch.cuda.empty_cache()\n",
    "        n_features = X.shape[1]\n",
    "        model = Classifier(n_features=n_features, n_hidden=n_hid,n_class=n_classes)\n",
    "        device = torch.device(f'cuda:{gpu}')\n",
    "        set_seed(seed)\n",
    "        model.to(device)\n",
    "\n",
    "        # Define loss function and optimizer\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\n",
    "        if method in meta_methods:\n",
    "            new_nego_rec = []\n",
    "            nego_sim = []\n",
    "\n",
    "        # Training loop\n",
    "        epoch_AUC, group_AUC, group_TPR, group_FPR, group_F1 = [], [], [], [], []\n",
    "        F_group_AUC, S_group_AUC = [], []\n",
    "        \n",
    "        for ep in tqdm(range(1, args.res_epochs+1)):\n",
    "            model.train()  # Set model to training mode\n",
    "            \n",
    "            for _, (inputs, labels, ind) in enumerate(train_loader):\n",
    "                inputs, labels = inputs.to(device=device, non_blocking=True),\\\n",
    "                                labels.to(device=device, non_blocking=True)\n",
    "\n",
    "                if 'nash' in method:\n",
    "                    #### Convex Optimization Problem (bargaining game) Initialization ####\n",
    "                    # Optimization Variable (current alpha)\n",
    "                    alpha_param = cp.Variable(shape=(bargain_groups,), nonneg=True) # shape: [K,]\n",
    "                    # G^T G: gradient matrix product\n",
    "                    init_gtg = np.eye(bargain_groups) # shape: [K, K]\n",
    "                    G_param = cp.Parameter(\n",
    "                        shape=(bargain_groups, bargain_groups), value=init_gtg\n",
    "                        ) # will be updated in-loop with the current GTG\n",
    "                    # Normalization parameter for the Phi_alpha_(tao)\n",
    "                    normalization_factor_param = cp.Parameter(\n",
    "                        shape=(1,), value=np.array([1.0])\n",
    "                        ) # will be updated in-loop with torch.norm(GTG).detach().cpu().numpy().reshape((1,))\n",
    "                    # Alpha from iteration 'Tao'\n",
    "                    prvs_alpha = np.ones(bargain_groups, dtype=np.float32)\n",
    "                    prvs_alpha_param = cp.Parameter(\n",
    "                        shape=(bargain_groups,), value=prvs_alpha\n",
    "                        ) # shape: [K,]\n",
    "                    # First-order approximation of Phi_alpha using Phi_alpha_(tao)\n",
    "                    G_prvs_alpha = G_param @ prvs_alpha_param\n",
    "                    prvs_phi_tag = 1 / prvs_alpha_param + (1 / G_prvs_alpha) @ G_param\n",
    "                    phi_alpha = prvs_phi_tag @ (alpha_param - prvs_alpha_param)\n",
    "                    # Beta(alpha)\n",
    "                    G_alpha = G_param @ alpha_param\n",
    "                    # Constraint: For any i, Phi_i_alpha >= 0\n",
    "                    constraint = []\n",
    "                    for i in range(bargain_groups):\n",
    "                        constraint.append(\n",
    "                            -cp.log(alpha_param[i] * normalization_factor_param)\n",
    "                            - cp.log(G_alpha[i])\n",
    "                            <= 0\n",
    "                        )\n",
    "                    obj = cp.Minimize(\n",
    "                        cp.sum(G_alpha) + phi_alpha / normalization_factor_param\n",
    "                    )\n",
    "                    prob = cp.Problem(obj, constraint)\n",
    "                    #####################################################\n",
    "\n",
    "                if method in meta_methods:\n",
    "                    # 0. entering the meta-loop for calculating the meta-weights for the outer\n",
    "                    with higher.innerloop_ctx(model, optimizer) as (meta_model, meta_opt):\n",
    "                        # 1. Update meta model on training data\n",
    "                        meta_train_outputs = meta_model(inputs)\n",
    "                        criterion.reduction = 'none'\n",
    "                        meta_train_loss = criterion(meta_train_outputs, labels.to(torch.int64))\n",
    "\n",
    "                        eps = torch.zeros(meta_train_loss.size(), requires_grad=True, device=device)\n",
    "\n",
    "                        meta_train_loss = torch.sum(eps * meta_train_loss)\n",
    "                        meta_opt.step(meta_train_loss)\n",
    "\n",
    "                        # 2. Compute grads of eps on meta validation data\n",
    "                        meta_inputs, meta_labels =  torch.from_numpy(X_val).float().to(device), torch.from_numpy(y_val).float().to(device)\n",
    "                        meta_inputs, meta_labels = meta_inputs.to(device=device, non_blocking=True),\\\n",
    "                                            meta_labels.to(device=device, non_blocking=True)\n",
    "\n",
    "                        meta_val_outputs = meta_model(meta_inputs)\n",
    "                        criterion.reduction = 'none'\n",
    "                        meta_val_loss = criterion(meta_val_outputs, meta_labels.to(torch.int64))\n",
    "                        meta_val_loss += F1Loss(meta_val_outputs, meta_labels.to(torch.int64))\n",
    "                        bargain_group_loss = meta_val_loss.view(-1,val_size//bargain_groups).mean(axis=1)\n",
    "                        grads = {}\n",
    "                        for sensi_idx in range(bargain_groups):\n",
    "                            grads[sensi_idx] = torch.autograd.grad(bargain_group_loss[sensi_idx], eps, create_graph=True)[0].detach()\n",
    "                        ###################\n",
    "                        \n",
    "                        if method == 'loss':\n",
    "                            eps_grads = torch.autograd.grad(meta_val_loss.mean(), eps)[0].detach()\n",
    "                        elif method == 'FORML':\n",
    "                            val_group_loss = meta_val_loss.view(-1,val_size//len(unique_groups)).mean(axis=1)\n",
    "                            val_dif_loss = val_group_loss.max() - val_group_loss.min()\n",
    "                            eps_grads = torch.autograd.grad(val_dif_loss, eps)[0].detach()\n",
    "                        elif method == 'meta_group_dro':\n",
    "                            val_group_loss = meta_val_loss.view(-1,val_size//len(unique_groups)).mean(axis=1)\n",
    "                            val_dif_loss = val_group_loss.max()\n",
    "                            eps_grads = torch.autograd.grad(val_dif_loss, eps)[0].detach()\n",
    "                        elif 'nash' in method:\n",
    "                            if ep<args.bargain_epochs:\n",
    "                                prvs_alpha = return_weights(grads, prvs_alpha)\n",
    "                                if np.all(prvs_alpha == 1):\n",
    "                                    # Bargaining failed, initiate default\n",
    "                                    if method == 'nash_meta_group_dro':\n",
    "                                        nash_val_loss = meta_val_loss.view(-1,val_size//(len(unique_groups))).mean(axis=1).max()\n",
    "                                        eps_grads = torch.autograd.grad(nash_val_loss, eps)[0].detach()\n",
    "                                    elif method == 'nash_FORML':\n",
    "                                        val_group_loss = meta_val_loss.view(-1,val_size//len(unique_groups)).mean(axis=1)\n",
    "                                        nash_val_loss = val_group_loss.max() - val_group_loss.min()\n",
    "                                        eps_grads = torch.autograd.grad(nash_val_loss, eps)[0].detach()\n",
    "                                else:\n",
    "                                    nash_val_loss = sum([bargain_group_loss[i] * prvs_alpha[i] for i in range(len(prvs_alpha))])\n",
    "                                    eps_grads = torch.autograd.grad(nash_val_loss, eps)[0].detach()\n",
    "                            else:\n",
    "                                if method == 'nash_loss':\n",
    "                                    eps_grads = torch.autograd.grad(meta_val_loss.mean(), eps)[0].detach()\n",
    "                                elif method == 'nash_meta_group_dro':\n",
    "                                    val_group_loss = meta_val_loss.view(-1,val_size//len(unique_groups)).mean(axis=1)\n",
    "                                    val_dif_loss = val_group_loss.max()\n",
    "                                    eps_grads = torch.autograd.grad(val_dif_loss, eps)[0].detach()\n",
    "                                elif method == 'nash_FORML':\n",
    "                                    val_group_loss = meta_val_loss.view(-1,val_size//len(unique_groups)).mean(axis=1)\n",
    "                                    val_dif_loss = val_group_loss.max() - val_group_loss.min()\n",
    "                                    eps_grads = torch.autograd.grad(val_dif_loss, eps)[0].detach()\n",
    "\n",
    "                        else:\n",
    "                            raise NotImplementedError\n",
    "                        bargaining_fail,_ = bargaining_fail_eval(grads, eps_grads)\n",
    "                        new_nego_rec.append(bargaining_fail)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(inputs)\n",
    "                criterion.reduction = 'none'\n",
    "                minibatch_loss = criterion(outputs, labels.to(torch.int64))\n",
    "\n",
    "\n",
    "                # 3. Compute weights for current training batch\n",
    "                if method in classic_methods:\n",
    "                    w_tilde = torch.ones_like(minibatch_loss)\n",
    "                else:\n",
    "                    w_tilde = torch.clamp(-eps_grads, min=0)\n",
    "\n",
    "                l2_norm = torch.norm(w_tilde)\n",
    "                if l2_norm != 0:\n",
    "                    w = w_tilde / l2_norm\n",
    "                else:\n",
    "                    w = w_tilde\n",
    "\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                if method == 'DRO':\n",
    "                    k = int(len(inputs)*0.4)\n",
    "                    top_k_losses,_ = torch.topk(minibatch_loss, k)\n",
    "                    loss = top_k_losses.sum()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                # Backward pass and optimization\n",
    "                else:\n",
    "                    minibatch_loss = torch.sum(w * minibatch_loss)\n",
    "                    minibatch_loss.backward()\n",
    "                    optimizer.step()\n",
    "                \n",
    "            # inference after each epoch\n",
    "            model.eval()\n",
    "            with torch.no_grad(): \n",
    "                y_pred = model(torch.from_numpy(X_test).float().to(device))\n",
    "                y_pred_labels = y_pred.argmax(dim=1).detach().cpu().numpy()\n",
    "                print(f'|Test AUC: {roc_auc_score(y_test, y_pred_labels):.3f}|', end='\\r')\n",
    "                epoch_AUC.append(roc_auc_score(y_test, y_pred_labels))\n",
    "\n",
    "                # Fair Metrics\n",
    "                AUC, TPR, FPR, F1 = fair_eval(y_test,y_pred_labels,unique_groups)\n",
    "                group_AUC.append(AUC)\n",
    "                group_TPR.append(TPR)\n",
    "                group_FPR.append(FPR)\n",
    "                group_F1.append(F1)\n",
    "                \n",
    "        weight_bargain_all[method] = weight_bargain\n",
    "        weight_post_bargain_all[method] = weight_post_bargain\n",
    "        suweight_bargain_all[method] = suweight_bargain\n",
    "        suweight_post_bargain_all[method] = suweight_post_bargain\n",
    "        \n",
    "        # Save metrics\n",
    "        metrics_to_save = {\n",
    "            'epoch_AUC': np.asarray(epoch_AUC),\n",
    "            'group_AUC': np.asarray(group_AUC),\n",
    "            'group_TPR': np.asarray(group_TPR),\n",
    "            'group_FPR': np.asarray(group_FPR),\n",
    "            'group_F1': np.asarray(group_F1)\n",
    "        }\n",
    "        if method in meta_methods:\n",
    "            save_metrics(method, metrics_to_save, meta_methods, new_nego_rec=np.asarray(new_nego_rec))\n",
    "        else:\n",
    "            save_metrics(method, metrics_to_save, meta_methods)\n",
    "        \n",
    "        if method in meta_methods:\n",
    "            new_nego_recs.append(new_nego_rec)\n",
    "            nego_sims.append(nego_sim)\n",
    "        epoch_AUCs.append(epoch_AUC)\n",
    "        group_AUCs.append(group_AUC)\n",
    "        \n",
    "\n",
    "    print('\\nFinal Results of the {}:'.format(method))\n",
    "    print('Overall AUC:   {:.3f}±{:.3f}'.format(np.mean(np.asarray(epoch_AUCs)[:,-1]), np.std(np.asarray(epoch_AUCs)[:,-1])))\n",
    "    print('Group-wise AUC:')\n",
    "    AUCs = np.asarray(group_AUCs)[:,-1]\n",
    "    for i in range(len(np.mean(AUCs, axis=0))):\n",
    "        print('   {:.3f}±{:.3f} ---- {}'.format(np.mean(AUCs, axis=0)[i], np.std(AUCs, axis=0)[i], unique_groups[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metric_data(data, metric, method):\n",
    "    return np.array(data[method][metric])\n",
    "\n",
    "\n",
    "def load_and_compute_metrics(base_path, dataset, sensitive_attribute, methods, seeds):\n",
    "    # Initialize data structure\n",
    "    data = {method: {metric: [] for metric in [\"average_AUC\", \"TPRD\", \"group_AUC_disparity\", \"worst_group_AUC\"]} for method in methods}\n",
    "    # Load and compute metrics for each method and seed\n",
    "    for method in methods:\n",
    "        if method not in ['baseline',\"DRO\"]:\n",
    "                data[method][\"nego_success\"] = []\n",
    "        for seed in seeds:\n",
    "            # seed_path = os.path.join(base_path, dataset+\"_new\", sensitive_attribute, method, str(seed))\n",
    "            seed_path = os.path.join(base_path, dataset, sensitive_attribute, method, str(seed))\n",
    "            \n",
    "            # Load group metrics_new\n",
    "            group_metrics = {}\n",
    "            for metric in [\"AUC\", \"TPR\"]:\n",
    "                with open(os.path.join(seed_path, f\"group_{metric}.json\"), 'r') as file:\n",
    "                    group_metrics[metric] = np.array(json.load(file))\n",
    "            \n",
    "            # Load epoch AUC\n",
    "            with open(os.path.join(seed_path, \"epoch_AUC.json\"), 'r') as file:\n",
    "                epoch_AUC = json.load(file)\n",
    "            \n",
    "            # Store average AUC\n",
    "            data[method][\"average_AUC\"].append(epoch_AUC)\n",
    "            \n",
    "            if method not in ['baseline',\"DRO\"]:\n",
    "                # Load epoch negotiation successrate\n",
    "                with open(os.path.join(seed_path, \"nego_success.json\"), 'r') as file:\n",
    "                    nego_success = json.load(file)\n",
    "                data[method][\"nego_success\"].append(nego_success)\n",
    "            \n",
    "            \n",
    "            \n",
    "            # Initialize temporary lists for each metric\n",
    "            temp_TPRD = []\n",
    "            temp_group_AUC_disparity = []\n",
    "            temp_worst_group_AUC = []\n",
    "\n",
    "            # Compute and store other metrics for each epoch\n",
    "            for epoch in range(len(epoch_AUC)):\n",
    "                # TPRD (True Positive Rate Disparity)_new\n",
    "                # print(group_metrics['TPR'][epoch])\n",
    "                TPRD = group_metrics['TPR'][epoch].max() - group_metrics['TPR'][epoch].min()\n",
    "                temp_TPRD.append(TPRD)\n",
    "\n",
    "                # Group AUC Disparity\n",
    "                group_AUC_disparity = group_metrics['AUC'][epoch].max() - group_metrics['AUC'][epoch].min()\n",
    "                temp_group_AUC_disparity.append(group_AUC_disparity)\n",
    "\n",
    "                # Worst Group's AUC\n",
    "                worst_group_AUC = group_metrics['AUC'][epoch].min()\n",
    "                temp_worst_group_AUC.append(worst_group_AUC)\n",
    "            \n",
    "            # Append the temporary lists to the main data structure\n",
    "            data[method][\"TPRD\"].append(temp_TPRD)\n",
    "            data[method][\"group_AUC_disparity\"].append(temp_group_AUC_disparity)\n",
    "            data[method][\"worst_group_AUC\"].append(temp_worst_group_AUC)\n",
    "    \n",
    "    # Convert lists to arrays for easier manipulation\n",
    "    for method in methods:\n",
    "        for metric in data[method]:\n",
    "            data[method][metric] = np.array(data[method][metric])\n",
    "\n",
    "    return data\n",
    "\n",
    "# Load data\n",
    "base_path = \"./results\"\n",
    "seeds = range(args.num_seeds)\n",
    "data = load_and_compute_metrics(base_path, dataset, current_sensitive, methods, seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in [\"average_AUC\", \"group_AUC_disparity\", \"worst_group_AUC\"]:\n",
    "    for i in range(len(methods)):\n",
    "        method = methods[i]\n",
    "        metric = metric\n",
    "        metric_data = get_metric_data(data, metric, method)\n",
    "        \n",
    "        performace_data = get_metric_data(data, \"average_AUC\", method)\n",
    "        best_idx = np.argmax(performace_data.mean(axis=0))\n",
    "        print(method, metric, f'{metric_data.mean(axis=0)[best_idx]:.3f}', f'{metric_data.std(axis=0)[best_idx]:.3f}')\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prune_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
